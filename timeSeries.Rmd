---
title: "Time Series"
author: "John Tipton"
date: "August 24, 2015"
output: "pdf_document"
---
```{r}
## Load R packages and define helper functions
library(ggplot2, quietly = TRUE)
library(reshape2, quietly = TRUE)
library(grid, quietly = TRUE)
## Function to plot multiple ggplots on the same image
multiplot <- function(..., plotlist=NULL, cols) {
  require(grid)
	
	# Make a list from the ... arguments and plotlist
	plots <- c(list(...), plotlist)
	
	numPlots = length(plots)
	
	# Make the panel
	plotCols = cols                       # Number of columns of plots
	plotRows = ceiling(numPlots/plotCols) # Number of rows needed, calculated from # of cols
	
	# Set up the page
	grid.newpage()
	pushViewport(viewport(layout = grid.layout(plotRows, plotCols)))
	vplayout <- function(x, y)
		viewport(layout.pos.row = x, layout.pos.col = y)
	
	# Make each plot, in the correct location
	for (i in 1:numPlots) {
		curRow = ceiling(i/plotCols)
		curCol = (i-1) %% plotCols + 1
		print(plots[[i]], vp = vplayout(curRow, curCol ))
	}	
}

##
## function to simulate time series
##

simTimeSeries <- function(t, N, mu, s, phi){
  if(N == 1){
    y <- rep(0, t)                                      ## initialize container
    y[1] <- mu[1] + rnorm(1, 0, s)                      ## initailize time series at time 1
    for(i in 2:t){
      epsilon <- rnorm(1, 0, s)                         ## independent random error
      y[i] <- mu[i] + phi * y[i-1] + epsilon            ## autoregressive model
    }
  } else {
    y <- matrix(0, t, N)                                ## initialize container
    y[1, ] <- mu[1] + rnorm(N, 0, s)                    ## initailize time series at time 1
    for(i in 2:t){
      epsilon <- rnorm(N, 0, s)                         ## independent random error
      y[i, ] <- mu[i] + phi * y[i-1, ] + epsilon        ## autoregressive model
    }
  }
  return(y)
}

```
First, we start with the canonical difference equation for the time series autoregressive model of order 1 (AR(1))
\begin{align}
\label{ar}
y_t & = \mu_t + \phi y_{t-1} + \epsilon_t
\end{align}
where the time series observations for times $t = 1, \ldots, T$ are given by the vector $\mathbf{y} = \left( y_1, \ldots, y_T \right)$ where $y_t$ is the observation of the time serise at time $t$. The vector $\boldsymbol{\mu}$ is the temporal mean with $\mu_t$ representing the mean of the time series at time $t$. Often the mean is a trend or seasonal component like in the example below. The autoregressive parameter $\phi$ controls the strength of autocorrelation in the time series with $-1 < \phi < 1$ and the random error $\epsilon_t \sim N(0, \sigma^2)$ is independent for different times (i.e. the covariance $\mathrm{Cov}(\epsilon_t, \epsilon_{t+k}) = 0$ for $k \neq 0$). 

# Lets simulate some data here
```{r simData}
N <- 1                              ## pick the number of time series
t <- 1000                           ## pick a time series length
mu <- rep(0, t)
# mu <- sin(2 * pi * (1:t)/t)         ## pick a mean
s <- 0.25                           ## pick standard deviation
phi <- 0.90                         ## pick autocorrelation parameter

y <- simTimeSeries(t, N, mu, s, phi)
ggplot(data=data.frame(y=y, t=1:t), aes(y=y, x=t)) + 
  geom_line(alpha=1, colour="darkgrey") + 
  ggtitle("Plot of a single time series") + 
  theme(plot.title = element_text(size=18))

```

The expected value $\mathrm{E}(y_t)$ of the time series at time $t$ is 
\begin{align*}
\mathrm{E}(y_t) & = \mathrm{E}(\phi y_{t-1}) + \mathrm{E}(\epsilon_t) \\
& = \phi \mathrm{E}(y_{t-1}) + 0
\end{align*}
where, assuming a constant mean $\mu$ accross time we have
\begin{align*}
\mathrm{E}(y_t) & = \phi \mathrm{E}(y_{t-1})\\
\rightarrow
\mathrm{E}(y_t) (1 - \phi) & = 0\\
\rightarrow
\mathrm{E}(y_t) & = 0\\
\end{align*}
and assuming constant variance through time, the variance is 
\begin{align*}
\mathrm{Var}(y_t) & = \mathrm{Var}(\phi y_{t - 1} + \epsilon_t) \\
& = \mathrm{Var}(\phi y_{t - 1}) + \mathrm{Var}(\epsilon_t) + 2 \mathrm{Cov}(\phi y_{t - 1}, \epsilon_t) \\
& = \phi^2 \mathrm{Var}(y_{t - 1}) + \sigma^2  + 0 \\
\end{align*}

Then using our modeling assumption $\mathrm{Var}(y_t) = \mathrm{Var}(y_{t-1})$,
\begin{align*}
\mathrm{Var}(y_t) - \phi^2 \mathrm{Var}(y_t) & = \sigma^2 \\
% & = \phi^2 \left(\phi^2 \mathrm{Var}(y_{t - 2}) + \sigma^2 \right) + \sigma^2 \\
% & = \vdots \\
\end{align*}
gives the solution $\mathrm{Var}(y_t) = \frac{\sigma^2}{1 - \phi^2}$.
```{r properties}
N <- 10                               ## replicates N
y <- simTimeSeries(t, N, mu, s, phi)  ## simulate time series
## legend() add in legend for mean and y label
## Notice that the y axis is shrunk to 0

## 

mean_y <- apply(y, 1, mean)    ##  calculate the mean
var_y <-apply(y, 1, var)       ##  calculate the variance
sd_y <- apply(y, 1, sd)        ##  calculate the standard deviation

time_data <- data.frame(y=y, t=1:t)
melt_time <- melt(time_data, id="t")                        
summary_data <- data.frame(mean_y=mean_y, var_y=var_y, sd_y=sd_y, mu=mu, 
                        s=s, t=1:t)


## plot time series with mean and variance


plot_mean <- ggplot(data = melt_time, aes(y=value, x=t)) + 
  geom_line(alpha=1, colour="darkgrey") + 
  geom_line(data=summary_data, aes(y=mean_y, x=t, colour="empirical"), 
            alpha=0.75) + 
  geom_line(data=summary_data, aes(y=mu, x=t, colour="truth"), alpha=0.75, 
            lty=2, lwd=2) + 
  scale_colour_manual("Mean", labels=c("empirical", "truth"), 
                      values=c("empirical"="red","truth"="blue")) + 
  scale_y_continuous("y") + scale_x_continuous("t") +
  ggtitle(paste(min(N, 10), 
                "time series with empircal and true mean")) +
  theme(plot.title = element_text(size=18))
 
plot_sd <- ggplot(data = melt_time, aes(y=value, x=t)) + 
  geom_line(alpha=1, colour="darkgrey") + 
  geom_line(data=summary_data, aes(y=sd_y, x=t, colour="empirical"), 
            alpha=0.75) + 
  geom_line(data=summary_data, aes(y=s, x=t, colour="truth"), alpha=0.75, 
            lty=2, lwd=2) + 
  scale_colour_manual("Std Dev", labels=c("empirical", "truth"), 
                      values=c("empirical"="red","truth"="blue")) + 
  scale_y_continuous("y") + scale_x_continuous("t") + 
  ggtitle(paste(min(N, 10), 
                "time series with empirical and true standard deviation")) + 
  theme(plot.title = element_text(size=18))

## Plot using multiplot
multiplot(plot_mean, plot_sd, cols=1)
```

The covariance between observations $\mathrm{Cov}(y_t, y_{t+k})$ at times $k$ lags apart (assuming without loss of generality that $k>0$) is
\begin{align*}
\mathrm{Cov}(y_t, y_{t+k}) & = \mathrm{E}(y_t y_{t+k}) - \mathrm{E}(y_t) \mathrm{E}(y_{t+k}) \\ 
& = \mathrm{E}(y_t (\phi y_{t+k-1} + \epsilon_{t+k})) - 0 \\
& = \mathrm{E}(\phi y_t y_{t+k-1}) + \mathrm{E}(y_t \epsilon_{t+k}) \\
& = \mathrm{E}(\phi y_t y_{t+k-1}) + \mathrm{E}(y_t) \mathrm{E}(\epsilon_{t+k})\\
& = \mathrm{E}(\phi y_t y_{t+k-1}) + 0 \\
& = \mathrm{E}(y_t (\phi y_{t+k-2} + \epsilon_{t+k-1})) \\
& = \vdots \\
& = \phi^{k} \mathrm{E}(y_t^2) \\
& = \phi^{k} \frac{\sigma^2}{1 - \phi^2}. \\
\end{align*}

```{r covariance}
## detrend the time series <- if you set mu = something other than 0
# y_detrend <- y-mu
# ggplot(data=melt(data.frame(y=y_detrend, t=1:t), id="t"), aes(y=value, x=t)) + 
#   geom_line(alpha=1, colour="darkgrey") + 
#   scale_y_continuous("y") + scale_x_continuous("t") +
#   ggtitle(paste(min(N, 10), 
#                 "time series with trend removed")) + 
#   theme(plot.title = element_text(size=18))

covariances <- matrix(0, min(t, 20), N)
for(i in 1:N){
    for(k in 1:min(t, 20)-1){
      covariances[k+1, i] <- cov(y[1:(t-k), i], y[1:(t-k) + k, i])
  }
}

cov_data <- data.frame(y=covariances, t=1:20)
melt_cov <- melt(cov_data, id="t")
ggplot(data = melt_cov, aes(y=value, x=t)) + 
  geom_line(data=melt_cov, aes(y=value, x=t, group=variable, colour="empirical"), alpha=1) + 
  geom_line(data=data.frame(y=s^2/(1-phi^2) * phi^(0:min(t,20)), x=0:min(t, 20)), aes(y=y, x=x, colour="truth"), 
            alpha=0.75) + 
  scale_colour_manual("Autocovariance", labels=c("empirical", "true"), 
                      values=c("empirical"="darkgrey","truth"="blue")) + 
  scale_y_continuous("autocovariance") + scale_x_continuous("lag") + 
  ggtitle(paste(min(N, 10), 
                "empirical autocovariance functions")) + 
  theme(plot.title = element_text(size=18))
```

Thus, knowing the mean, variance, and covariance at each time $t$ and each lag $k$, we can write the autoregressive model (\ref{ar1}) as 
\begin{align}
\label{var}
\mathbf{y} & = \boldsymbol{\mu} + \boldsymbol{\eta}
\end{align}
where $\mathbf{\mu} = \left(0, \ldots, 0 \right)$ and $\eta ~ \mbox{N}\left( \mathbf{0}, \boldsymbol{\Sigma} \right)$ where
\begin{align*}
\boldsymbol{\Sigma} & = \frac{\sigma^2}{1 - \phi^2} \left(\begin{pmatrix}
1 & \phi & \phi^2 & \phi^3 & \cdots & \phi^{T-1}\\
\phi & 1 & \phi & \phi^2 & \cdots & \phi^{T-2}\\
\phi^2 & \phi & 1 & \phi & \cdots & \phi^{T-3} \\
\phi^3 & \phi^2 & \phi & 1 & \cdots & \phi^{T-4} \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
\phi^{T-1} & \phi^{T-2} & \phi^{T-3} & \phi^{T-4} & \cdots & 1
\end{pmatrix} \right)
\end{align*}

```{r vectorTimeSeries}
Sigma <- s^2 / (1 - phi^2) * toeplitz(phi^(0:(t-1)))
library(mvtnorm)
y_vec <- t(rmvnorm(N, mu, Sigma))

vec_data <- data.frame(y=y_vec, t=1:t)
melt_vec <- melt(vec_data, id="t")                        


ggplot(data = melt_vec, aes(y=value, x=t)) + 
  geom_line(alpha=1, colour="darkgrey") + 
  scale_y_continuous("y") + scale_x_continuous("t") + 
  ggtitle(paste(min(N, 10), 
                "time series simulated as a vector")) + 
  theme(plot.title = element_text(size=18))
```

Typically, the distributions of interest in a time series model are the forecast distribution (used for prediction) and the smoothing distribution (used for estimation of parameters). The forecast distribution at time $\tau + 1$ consists of knowledge of all of the observations of the time series up to the time $\tau$ $y_{1:{\tau}} = (y_1, \ldots, y_{\tau})$ given by
\begin{align*}
[y_{\tau+1} | y_{1:{\tau}}] & = [y_{\tau+1} | y_{\tau}]
\end{align*}
by the Markov assumption in the autoregressive model. Then, the one step ahead expected forecast is 
\begin{align*}
\mathrm{E}(y_{\tau+1} | y_{1:{\tau}}) & = \mathrm{E}(y_{\tau+1} | y_{\tau}) \\
& = \mathrm{E}(\phi y_{\tau} + \epsilon_{\tau + 1} | y_{\tau}) \\
& = \mathrm{E}(\phi y_{\tau}| y_{\tau}) + \mathrm{E}(\epsilon_{\tau+1} | y_{\tau}) \\
& \phi y_{\tau} + 0. \\
\end{align*}
The $k$ step ahead expected forecast is calculated by using a recursive formula of the equation above where $\mathrm{E}(y_{\tau + k} | y_{1:{\tau}}) = \phi^k y_{\tau}$.

Likewise, the one step ahead forecast variance is  
\begin{align*}
\mathrm{Var}(y_{\tau+1} | y_{1:{\tau}}) & = \mathrm{Var}(y_{\tau+1} | y_{\tau}) \\
& = \mathrm{Var}(\phi y_{\tau} + \epsilon_{\tau+1} | y_{\tau}) \\
& = \phi^2 \mathrm{Var}(y_{\tau}| y_{\tau}) + 2 \mathrm{Cov}(\phi y_{\tau}, \epsilon_{\tau+1}| y_{\tau}) + \mathrm{Var}(\epsilon_{\tau+1} | y_{\tau}) \\
& =  0 + 0 + \sigma^2.
\end{align*}
The $k$ step ahead forecast variance can also be calcuated recursively giving $\mathrm{Var}(y_{\tau+k} | y_{1:{\tau-1}}) = \sum_{i=1}^k \phi^{2(i-1)} \sigma^2$.

